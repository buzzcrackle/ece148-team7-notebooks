{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on https://visualstudiomagazine.com/articles/2020/11/24/pytorch-accuracy.aspx\n",
    "\n",
    "# banknote_bnn.py\n",
    "# Banknote classification\n",
    "# PyTorch 1.6.0-CPU Anaconda3-2020.02  Python 3.7.6\n",
    "# Windows 10 \n",
    "\n",
    "import os\n",
    "import glob\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import normalize\n",
    "import csv\n",
    "import pandas as pd\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from joblib import dump, load\n",
    "\n",
    "import numpy as np\n",
    "import torch as T\n",
    "device = T.device(\"cuda\")  # apply to Tensor or Module\n",
    "T.set_default_tensor_type(T.cuda.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = ['not_crossing_wen', 'not_crossing_nick', 'not_crossing']\n",
    "locations_crossing = ['crossing_wen', 'crossing_nick', 'crossing']\n",
    "\n",
    "not_crossing = defaultdict(list)\n",
    "crossing = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read only the valid data into dict\n",
    "\n",
    "for location in locations:\n",
    "    path = 'data/' + location + '/'\n",
    "    for filename in glob.glob(path + '*.csv'):\n",
    "        name = filename.replace(path, '')[:-4]\n",
    "\n",
    "        df=pd.read_csv(filename)\n",
    "        if len(df.columns) >= 5 * 2: # 2 columns per point for x & y\n",
    "            for data in df.values:\n",
    "                not_crossing[name].append(data)\n",
    "                \n",
    "for location in locations_crossing:\n",
    "    path = 'data/' + location + '/'\n",
    "    for filename in glob.glob(path + '*.csv'):\n",
    "        name = filename.replace(path, '')[:-4]\n",
    "\n",
    "        df=pd.read_csv(filename)\n",
    "        if len(df.columns) >= 5 * 2: # 2 columns per point for x & y\n",
    "            for data in df.values:\n",
    "                crossing[name].append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"567891011121314151617\"\n",
    "\n",
    "cross_arr = np.asarray(crossing[file])\n",
    "cross_arr = [np.append(d, 0) for d in cross_arr]\n",
    "\n",
    "not_cross_arr = np.asarray(not_crossing[file])\n",
    "not_cross_arr = [np.append(d, 1) for d in not_cross_arr]\n",
    "\n",
    "combined_arr = cross_arr + not_cross_arr\n",
    "\n",
    "random.shuffle(combined_arr)\n",
    "\n",
    "N = len(combined_arr)\n",
    "\n",
    "train_arr = combined_arr[:4*N//5]\n",
    "test_arr = combined_arr[4*N//5:]\n",
    "\n",
    "np.savetxt('train.csv', train_arr, delimiter=',')\n",
    "np.savetxt('test.csv', test_arr, delimiter=',')\n",
    "\n",
    "num_features = len(cross_arr[0])-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BestDataset(T.utils.data.Dataset):\n",
    "\n",
    "\tdef __init__(self, src_file, num_rows=None):\n",
    "\t\tall_data = np.loadtxt(src_file, delimiter=\",\", skiprows=0,\n",
    "\t\t\tdtype=np.float32)  # strip IDs off\n",
    "\t\tall_data = np.loadtxt(open(src_file, \"rb\"), delimiter=\",\", skiprows=0)\n",
    "\t\tprint(len(all_data[0]))\n",
    "\t\tself.x_data = T.tensor(all_data[:,0:num_features],\n",
    "\t\t\tdtype=T.float32).to(device)\n",
    "\t\tself.y_data = T.tensor(all_data[:,num_features],\n",
    "\t\t\tdtype=T.float32).to(device)\n",
    "\n",
    "\t\t# n_vals = len(self.y_data)\n",
    "\t\t# self.y_data = self.y_data.reshape(n_vals,1)\n",
    "\t\tself.y_data = self.y_data.reshape(-1,1)\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.x_data)\n",
    "\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\tif T.is_tensor(idx):\n",
    "\t\t\tidx = idx.tolist()\n",
    "\t\tpreds = self.x_data[idx,:]  # idx rows, all 4 cols\n",
    "\t\tlbl = self.y_data[idx,:]    # idx rows, the 1 col\n",
    "\t\tsample = { 'predictors' : preds, 'target' : lbl }\n",
    "\t\t# sample = dict()   # or sample = {}\n",
    "\t\t# sample[\"predictors\"] = preds\n",
    "\t\t# sample[\"target\"] = lbl\n",
    "# \t\tprint(sample)\n",
    "\t\treturn sample\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "def accuracy(model, ds):\n",
    "\t# ds is a iterable Dataset of Tensors\n",
    "\tn_correct = 0; n_wrong = 0\n",
    "\n",
    "\t# alt: create DataLoader and then enumerate it\n",
    "\tfor i in range(len(ds)):\n",
    "\t\tinpts = ds[i]['predictors']\n",
    "\t\ttarget = ds[i]['target']    # float32  [0.0] or [1.0]\n",
    "\t\twith T.no_grad():\n",
    "\t\t\toupt = model(inpts)\n",
    "\n",
    "\t\t# avoid 'target == 1.0'\n",
    "\t\tif target < 0.5 and oupt < 0.5:  # .item() not needed\n",
    "\t\t\tn_correct += 1\n",
    "\t\telif target >= 0.5 and oupt >= 0.5:\n",
    "\t\t\tn_correct += 1\n",
    "\t\telse:\n",
    "\t\t\tn_wrong += 1\n",
    "\n",
    "\treturn (n_correct * 1.0) / (n_correct + n_wrong)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "def acc_coarse(model, ds):\n",
    "\tinpts = ds[:]['predictors']  # all rows\n",
    "\ttargets = ds[:]['target']    # all target 0s and 1s\n",
    "\twith T.no_grad():\n",
    "\t\toupts = model(inpts)         # all computed ouputs\n",
    "\tpred_y = oupts >= 0.5        # tensor of 0s and 1s\n",
    "\tnum_correct = T.sum(targets==pred_y)\n",
    "\tacc = (num_correct.item() * 1.0 / len(ds))  # scalar\n",
    "\treturn acc\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "def my_bce(model, batch):\n",
    "\t# mean binary cross entropy error. somewhat slow\n",
    "\tsum = 0.0\n",
    "\tinpts = batch['predictors']\n",
    "\ttargets = batch['target']\n",
    "\twith T.no_grad():\n",
    "\t\toupts = model(inpts)\n",
    "\tfor i in range(len(inpts)):\n",
    "\t\toupt = oupts[i]\n",
    "\t\t# should prevent log(0) which is -infinity\n",
    "\t\tif targets[i] >= 0.5:  # avoiding == 1.0\n",
    "\t\t\tsum += T.log(oupt)\n",
    "\t\telse:\n",
    "\t\t\tsum += T.log(1 - oupt)\n",
    "\n",
    "\treturn -sum / len(inpts)\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "class Net(T.nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(Net, self).__init__()\n",
    "\t\tself.hid1 = T.nn.Linear(num_features, 8)  # 4-(8-8)-1\n",
    "\t\tself.hid2 = T.nn.Linear(8, 8)\n",
    "\t\tself.oupt = T.nn.Linear(8, 1)\n",
    "\n",
    "\t\tT.nn.init.xavier_uniform_(self.hid1.weight) \n",
    "\t\tT.nn.init.zeros_(self.hid1.bias)\n",
    "\t\tT.nn.init.xavier_uniform_(self.hid2.weight) \n",
    "\t\tT.nn.init.zeros_(self.hid2.bias)\n",
    "\t\tT.nn.init.xavier_uniform_(self.oupt.weight) \n",
    "\t\tT.nn.init.zeros_(self.oupt.bias)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tz = T.tanh(self.hid1(x)) \n",
    "\t\tz = T.tanh(self.hid2(z))\n",
    "\t\tz = T.sigmoid(self.oupt(z)) \n",
    "\t\treturn z\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "def main():\n",
    "\t# 0. get started\n",
    "\tprint(\"\\nBanknote authentication using PyTorch \\n\")\n",
    "\tT.manual_seed(1)\n",
    "\tnp.random.seed(1)\n",
    "\n",
    "\t# 1. create Dataset and DataLoader objects\n",
    "\tprint(\"Creating Banknote train and test DataLoader \")\n",
    "\n",
    "# \ttrain_file = \"./banknote_k20_train.txt\"\n",
    "# \ttest_file = \"./banknote_k20_test.txt\"\n",
    "\n",
    "# \ttrain_ds = BanknoteDataset(train_file)  # all rows\n",
    "# \ttest_ds = BanknoteDataset(test_file)\n",
    "\n",
    "\ttrain_ds = BestDataset(\"train.csv\")\n",
    "\ttest_ds = BestDataset(\"test.csv\")# all rows\n",
    "\n",
    "\tbat_size = 10\n",
    "\ttrain_ldr = T.utils.data.DataLoader(train_ds,\n",
    "\t\tbatch_size=bat_size, shuffle=True)\n",
    "\t# test_ldr = T.utils.data.DataLoader(test_ds,\n",
    "\t#   batch_size=1, shuffle=False)  # not needed\n",
    "\n",
    "\t# 2. create neural network\n",
    "\tprint(\"Creating 4-(8-8)-1 binary NN classifier \")\n",
    "\tnet = Net().to(device)\n",
    "\n",
    "\t# 3. train network\n",
    "\tprint(\"\\nPreparing training\")\n",
    "\tnet = net.train()  # set training mode\n",
    "\tlrn_rate = 0.01\n",
    "\tloss_obj = T.nn.BCELoss()  # binary cross entropy\n",
    "\toptimizer = T.optim.SGD(net.parameters(),\n",
    "\t\tlr=lrn_rate)\n",
    "\tmax_epochs = 500\n",
    "\tep_log_interval = 10\n",
    "\tprint(\"Loss function: \" + str(loss_obj))\n",
    "\tprint(\"Optimizer: SGD\")\n",
    "\tprint(\"Learn rate: 0.01\")\n",
    "\tprint(\"Batch size: 10\")\n",
    "\tprint(\"Max epochs: \" + str(max_epochs))\n",
    "\n",
    "\tprint(\"\\nStarting training\")\n",
    "\tfor epoch in range(0, max_epochs):\n",
    "\t\tepoch_loss = 0.0            # for one full epoch\n",
    "\t\tepoch_loss_custom = 0.0\n",
    "\t\tnum_lines_read = 0\n",
    "\n",
    "\t\tfor (batch_idx, batch) in enumerate(train_ldr):\n",
    "\t\t\tX = batch['predictors']  # [10,4]  inputs\n",
    "\t\t\tY = batch['target']      # [10,1]  targets\n",
    "\t\t\toupt = net(X)            # [10,1]  computeds \n",
    "\n",
    "\t\t\tloss_val = loss_obj(oupt, Y)   # a tensor\n",
    "\t\t\tepoch_loss += loss_val.item()  # accumulate\n",
    "\t\t\t# epoch_loss += loss_val  # is OK\n",
    "\t\t\t# epoch_loss_custom += my_bce(net, batch)\n",
    "\n",
    "\t\t\toptimizer.zero_grad() # reset all gradients\n",
    "\t\t\tloss_val.backward()   # compute all gradients\n",
    "\t\t\toptimizer.step()      # update all weights\n",
    "\n",
    "\t\tif epoch % ep_log_interval == 0:\n",
    "\t\t\tprint(\"epoch = %4d   loss = %0.4f\" % \\\n",
    "\t\t\t\t(epoch, epoch_loss))\n",
    "\t\t\t# print(\"custom loss = %0.4f\" % epoch_loss_custom)\n",
    "\t\t\t# print(\"\")\n",
    "\tprint(\"Done \")\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "\t# 4. evaluate model\n",
    "\tnet = net.eval()\n",
    "\tacc_train = accuracy(net, train_ds)\n",
    "\tprint(\"\\nAccuracy on train data = %0.2f%%\" % \\\n",
    "\t\t(acc_train * 100))\n",
    "\tacc_test = accuracy(net, test_ds)\n",
    "\tprint(\"Accuracy on test data = %0.2f%%\" % \\\n",
    "\t\t(acc_test * 100))\n",
    "\n",
    "\t# acc_train_c = acc_coarse(net, train_ds)\n",
    "\t# print(\"Accuracy on train data = %0.2f%%\" % \\\n",
    "\t#  (acc_train_c * 100))\n",
    "\t# acc_test_c = acc_coarse(net, test_ds)\n",
    "\t# print(\"Accuracy on test data = %0.2f%%\" % \\\n",
    "\t#  (acc_test_c * 100))\n",
    "\n",
    "\t# 5. save model\n",
    "\tprint(\"\\nSaving trained model state_dict \\n\")\n",
    "\tpath = \"./Models/banknote_sd_model.pth\"\n",
    "\tT.save(net.state_dict(), path)\n",
    "\n",
    "\t# print(\"\\nSaving entire model \\n\")\n",
    "\t# path = \".\\\\Models\\\\banknote_full_model.pth\"\n",
    "\t# T.save(net, path\n",
    "\n",
    "\t# print(\"\\nSaving trained model as ONNX \\n\")\n",
    "\t# path = \".\\\\Models\\\\banknote_onnx_model.onnx\"\n",
    "\t# dummy = T.tensor([[0.5, 0.5, 0.5, 0.5]],\n",
    "\t#   dtype=T.float32).to(device)\n",
    "\t# T.onnx.export(net, dummy, path,\n",
    "\t#   input_names=[\"input1\"],\n",
    "\t#  output_names=[\"output1\"])\n",
    "\n",
    "\t# model = Net()  # later . . \n",
    "\t# model.load_state_dict(T.load(path))\n",
    "\n",
    "\t# 6. make a prediction \n",
    "\traw_inpt = np.array([[4.670006930828095038e-01,2.772299349308013916e-01,4.581054151058197021e-01,2.819681465625762939e-01,4.726324081420897882e-01,3.808141648769378662e-01,4.454198181629180908e-01,3.854113519191741943e-01,4.704093933105468750e-01,4.648146331310272217e-01,4.108781218528748114e-01,4.509174823760985773e-01,4.516118168830872137e-01,4.790165424346923828e-01,4.657082557678223211e-01,4.795077741146088202e-01,4.269051551818848211e-01,6.205608248710632324e-01,4.691285192966461182e-01,6.179753541946411133e-01,4.600642025470734198e-01,7.593259215354919434e-01,4.798959791660308838e-01,7.504850029945373535e-01,4.634187221527100164e-01,2.808338105678558350e-01]],\n",
    "\t\tdtype=np.float32) # should be crossing\n",
    "# \tnorm_inpt = raw_inpt / 20\n",
    "\tunknown = T.tensor(raw_inpt,\n",
    "\t\tdtype=T.float32).to(device) \n",
    "\n",
    "\tprint(\"Setting normalized inputs to:\")\n",
    "\tfor x in unknown[0]:\n",
    "\t\tprint(\"%0.3f \" % x, end=\"\")\n",
    "\n",
    "\tnet = net.eval()\n",
    "\twith T.no_grad():\n",
    "\t\traw_out = net(unknown)    # a Tensor\n",
    "\tpred_prob = raw_out.item()  # scalar, [0.0, 1.0]\n",
    "\n",
    "\tprint(\"\\nPrediction prob = %0.6f \" % pred_prob)\n",
    "\tif pred_prob < 0.5:\n",
    "\t\tprint(\"Prediction = crossing\")\n",
    "\telse:\n",
    "\t\tprint(\"Prediction = not_crossing\")\n",
    "\n",
    "\tprint(\"\\nEnd Banknote demo\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Banknote authentication using PyTorch \n",
      "\n",
      "Creating Banknote train and test DataLoader \n",
      "27\n",
      "27\n",
      "Creating 4-(8-8)-1 binary NN classifier \n",
      "\n",
      "Preparing training\n",
      "Loss function: BCELoss()\n",
      "Optimizer: SGD\n",
      "Learn rate: 0.01\n",
      "Batch size: 10\n",
      "Max epochs: 500\n",
      "\n",
      "Starting training\n",
      "epoch =    0   loss = 85.5967\n",
      "epoch =   10   loss = 69.4232\n",
      "epoch =   20   loss = 51.9655\n",
      "epoch =   30   loss = 46.9999\n",
      "epoch =   40   loss = 45.3146\n",
      "epoch =   50   loss = 44.1043\n",
      "epoch =   60   loss = 44.0869\n",
      "epoch =   70   loss = 43.4022\n",
      "epoch =   80   loss = 42.2456\n",
      "epoch =   90   loss = 41.7404\n",
      "epoch =  100   loss = 39.9229\n",
      "epoch =  110   loss = 39.0081\n",
      "epoch =  120   loss = 38.5179\n",
      "epoch =  130   loss = 36.7395\n",
      "epoch =  140   loss = 35.9684\n",
      "epoch =  150   loss = 34.1167\n",
      "epoch =  160   loss = 32.0421\n",
      "epoch =  170   loss = 33.0791\n",
      "epoch =  180   loss = 32.5502\n",
      "epoch =  190   loss = 30.4329\n",
      "epoch =  200   loss = 29.3044\n",
      "epoch =  210   loss = 29.1486\n",
      "epoch =  220   loss = 27.0744\n",
      "epoch =  230   loss = 26.7962\n",
      "epoch =  240   loss = 26.3200\n",
      "epoch =  250   loss = 24.4505\n",
      "epoch =  260   loss = 26.0834\n",
      "epoch =  270   loss = 24.4106\n",
      "epoch =  280   loss = 24.9619\n",
      "epoch =  290   loss = 25.1979\n",
      "epoch =  300   loss = 24.8087\n",
      "epoch =  310   loss = 24.7313\n",
      "epoch =  320   loss = 22.4617\n",
      "epoch =  330   loss = 19.2721\n",
      "epoch =  340   loss = 24.9902\n",
      "epoch =  350   loss = 19.8518\n",
      "epoch =  360   loss = 21.1457\n",
      "epoch =  370   loss = 18.3176\n",
      "epoch =  380   loss = 18.3037\n",
      "epoch =  390   loss = 21.0641\n",
      "epoch =  400   loss = 17.5143\n",
      "epoch =  410   loss = 21.2877\n",
      "epoch =  420   loss = 19.2405\n",
      "epoch =  430   loss = 17.3354\n",
      "epoch =  440   loss = 17.8765\n",
      "epoch =  450   loss = 18.8486\n",
      "epoch =  460   loss = 19.2515\n",
      "epoch =  470   loss = 15.7608\n",
      "epoch =  480   loss = 18.2037\n",
      "epoch =  490   loss = 18.0185\n",
      "Done \n",
      "\n",
      "Accuracy on train data = 97.24%\n",
      "Accuracy on test data = 95.60%\n",
      "\n",
      "Saving trained model state_dict \n",
      "\n",
      "Setting normalized inputs to:\n",
      "0.467 0.277 0.458 0.282 0.473 0.381 0.445 0.385 0.470 0.465 0.411 0.451 0.452 0.479 0.466 0.480 0.427 0.621 0.469 0.618 0.460 0.759 0.480 0.750 0.463 0.281 \n",
      "Prediction prob = 0.991947 \n",
      "Prediction = not_crossing\n",
      "\n",
      "End Banknote demo\n"
     ]
    }
   ],
   "source": [
    "if __name__== \"__main__\":\n",
    "\tmain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction = not_crossing\n"
     ]
    }
   ],
   "source": [
    "# Below is code to load and use model in any python code, with dependencies and for CUDA of course\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "device = torch.device(\"cuda\")  # apply to Tensor or Module\n",
    "torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
    "\n",
    "num_features = 26\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.hid1 = torch.nn.Linear(num_features, 8)  # 4-(8-8)-1\n",
    "        self.hid2 = torch.nn.Linear(8, 8)\n",
    "        self.oupt = torch.nn.Linear(8, 1)\n",
    "\n",
    "        torch.nn.init.xavier_uniform_(self.hid1.weight) \n",
    "        torch.nn.init.zeros_(self.hid1.bias)\n",
    "        torch.nn.init.xavier_uniform_(self.hid2.weight) \n",
    "        torch.nn.init.zeros_(self.hid2.bias)\n",
    "        torch.nn.init.xavier_uniform_(self.oupt.weight) \n",
    "        torch.nn.init.zeros_(self.oupt.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = torch.tanh(self.hid1(x)) \n",
    "        z = torch.tanh(self.hid2(z))\n",
    "        z = torch.sigmoid(self.oupt(z)) \n",
    "        return z\n",
    "\n",
    "path = \"./Models/banknote_sd_model.pth\"\n",
    "model = Net()  # later . . \n",
    "model.load_state_dict(torch.load(path))\n",
    "model.eval()\n",
    "raw_inpt = np.array([[5.064111351966857910e-01,3.407363891601562500e-01,4.424785971641541082e-01,3.454131484031677246e-01,5.245444178581237793e-01,4.076796472072600763e-01,4.272206425666809082e-01,4.171701073646545410e-01,5.400559306144714355e-01,4.632427692413329523e-01,4.187252819538117010e-01,4.752222895622252863e-01,4.997749328613281250e-01,4.866386353969573975e-01,4.575124680995941162e-01,4.877757132053375244e-01,5.110443830490112305e-01,5.903016328811645508e-01,4.606600701808929998e-01,5.934466123580932617e-01,5.149625539779663086e-01,6.890525221824645996e-01,4.601623713970183771e-01,6.958268284797668457e-01,4.746918976306914728e-01,3.439677357673645020e-01]],\n",
    "    dtype=np.float32) # should be not_crossing\n",
    "inpt = torch.tensor(raw_inpt, dtype=torch.float32).to(device) \n",
    "raw_out = model(inpt)\n",
    "pred_prob = raw_out.item()\n",
    "if pred_prob < 0.5:\n",
    "    print(\"Prediction = crossing\")\n",
    "else:\n",
    "    print(\"Prediction = not_crossing\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
