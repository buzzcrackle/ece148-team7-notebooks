{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# banknote_bnn.py\n",
    "# Banknote classification\n",
    "# PyTorch 1.6.0-CPU Anaconda3-2020.02  Python 3.7.6\n",
    "# Windows 10 \n",
    "\n",
    "import os\n",
    "import glob\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import normalize\n",
    "import csv\n",
    "import pandas as pd\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from joblib import dump, load\n",
    "\n",
    "import numpy as np\n",
    "import torch as T\n",
    "device = T.device(\"cuda\")  # apply to Tensor or Module\n",
    "\n",
    "#        1         2         3         4         5         6\n",
    "# 3456789012345678901234567890123456789012345678901234567890\n",
    "# ----------------------------------------------------------\n",
    "# predictors and label in same file\n",
    "# archive.ics.uci.edu/ml/datasets/banknote+authentication\n",
    "# IDs 0001 to 1372 added\n",
    "# data has been k=20 normalized (all four columns)\n",
    "# ID  variance  skewness  kurtosis  entropy  class\n",
    "# [0]    [1]      [2]       [3]       [4]     [5]\n",
    "#  (0 = authentic, 1 = forgery)  # verified\n",
    "# train: 1097 items (80%), test: 275 item (20%)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = ['not_crossing_wen', 'not_crossing_nick', 'not_crossing']\n",
    "locations_crossing = ['crossing_wen', 'crossing_nick', 'crossing']\n",
    "\n",
    "not_crossing = defaultdict(list)\n",
    "crossing = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read only the valid data into dict\n",
    "\n",
    "for location in locations:\n",
    "    path = 'data/' + location + '/'\n",
    "    for filename in glob.glob(path + '*.csv'):\n",
    "        name = filename.replace(path, '')[:-4]\n",
    "\n",
    "        df=pd.read_csv(filename)\n",
    "        if len(df.columns) >= 5 * 2: # 2 columns per point for x & y\n",
    "            for data in df.values:\n",
    "                not_crossing[name].append(data)\n",
    "                \n",
    "for location in locations_crossing:\n",
    "    path = 'data/' + location + '/'\n",
    "    for filename in glob.glob(path + '*.csv'):\n",
    "        name = filename.replace(path, '')[:-4]\n",
    "\n",
    "        df=pd.read_csv(filename)\n",
    "        if len(df.columns) >= 5 * 2: # 2 columns per point for x & y\n",
    "            for data in df.values:\n",
    "                crossing[name].append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"01234567891011121314151617\"\n",
    "\n",
    "cross_arr = np.asarray(crossing[file])\n",
    "cross_arr = [np.append(d, 0) for d in cross_arr]\n",
    "\n",
    "not_cross_arr = np.asarray(not_crossing[file])\n",
    "not_cross_arr = [np.append(d, 1) for d in not_cross_arr]\n",
    "\n",
    "combined_arr = cross_arr + not_cross_arr\n",
    "\n",
    "random.shuffle(combined_arr)\n",
    "\n",
    "N = len(combined_arr)\n",
    "\n",
    "train_arr = combined_arr[:4*N//5]\n",
    "test_arr = combined_arr[4*N//5:]\n",
    "\n",
    "np.savetxt('train.csv', train_arr, delimiter=',')\n",
    "np.savetxt('test.csv', test_arr, delimiter=',')\n",
    "\n",
    "num_features = len(cross_arr[0])-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BestDataset(T.utils.data.Dataset):\n",
    "\n",
    "\tdef __init__(self, src_file, num_rows=None):\n",
    "\t\tall_data = np.loadtxt(src_file, delimiter=\",\", skiprows=0,\n",
    "\t\t\tdtype=np.float32)  # strip IDs off\n",
    "\t\tall_data = np.loadtxt(open(src_file, \"rb\"), delimiter=\",\", skiprows=0)\n",
    "\t\tprint(len(all_data[0]))\n",
    "\t\tself.x_data = T.tensor(all_data[:,0:num_features],\n",
    "\t\t\tdtype=T.float32).to(device)\n",
    "\t\tself.y_data = T.tensor(all_data[:,num_features],\n",
    "\t\t\tdtype=T.float32).to(device)\n",
    "\n",
    "\t\t# n_vals = len(self.y_data)\n",
    "\t\t# self.y_data = self.y_data.reshape(n_vals,1)\n",
    "\t\tself.y_data = self.y_data.reshape(-1,1)\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.x_data)\n",
    "\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\tif T.is_tensor(idx):\n",
    "\t\t\tidx = idx.tolist()\n",
    "\t\tpreds = self.x_data[idx,:]  # idx rows, all 4 cols\n",
    "\t\tlbl = self.y_data[idx,:]    # idx rows, the 1 col\n",
    "\t\tsample = { 'predictors' : preds, 'target' : lbl }\n",
    "\t\t# sample = dict()   # or sample = {}\n",
    "\t\t# sample[\"predictors\"] = preds\n",
    "\t\t# sample[\"target\"] = lbl\n",
    "# \t\tprint(sample)\n",
    "\t\treturn sample\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "def accuracy(model, ds):\n",
    "\t# ds is a iterable Dataset of Tensors\n",
    "\tn_correct = 0; n_wrong = 0\n",
    "\n",
    "\t# alt: create DataLoader and then enumerate it\n",
    "\tfor i in range(len(ds)):\n",
    "\t\tinpts = ds[i]['predictors']\n",
    "\t\ttarget = ds[i]['target']    # float32  [0.0] or [1.0]\n",
    "\t\twith T.no_grad():\n",
    "\t\t\toupt = model(inpts)\n",
    "\n",
    "\t\t# avoid 'target == 1.0'\n",
    "\t\tif target < 0.5 and oupt < 0.5:  # .item() not needed\n",
    "\t\t\tn_correct += 1\n",
    "\t\telif target >= 0.5 and oupt >= 0.5:\n",
    "\t\t\tn_correct += 1\n",
    "\t\telse:\n",
    "\t\t\tn_wrong += 1\n",
    "\n",
    "\treturn (n_correct * 1.0) / (n_correct + n_wrong)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "def acc_coarse(model, ds):\n",
    "\tinpts = ds[:]['predictors']  # all rows\n",
    "\ttargets = ds[:]['target']    # all target 0s and 1s\n",
    "\twith T.no_grad():\n",
    "\t\toupts = model(inpts)         # all computed ouputs\n",
    "\tpred_y = oupts >= 0.5        # tensor of 0s and 1s\n",
    "\tnum_correct = T.sum(targets==pred_y)\n",
    "\tacc = (num_correct.item() * 1.0 / len(ds))  # scalar\n",
    "\treturn acc\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "def my_bce(model, batch):\n",
    "\t# mean binary cross entropy error. somewhat slow\n",
    "\tsum = 0.0\n",
    "\tinpts = batch['predictors']\n",
    "\ttargets = batch['target']\n",
    "\twith T.no_grad():\n",
    "\t\toupts = model(inpts)\n",
    "\tfor i in range(len(inpts)):\n",
    "\t\toupt = oupts[i]\n",
    "\t\t# should prevent log(0) which is -infinity\n",
    "\t\tif targets[i] >= 0.5:  # avoiding == 1.0\n",
    "\t\t\tsum += T.log(oupt)\n",
    "\t\telse:\n",
    "\t\t\tsum += T.log(1 - oupt)\n",
    "\n",
    "\treturn -sum / len(inpts)\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "class Net(T.nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(Net, self).__init__()\n",
    "\t\tself.hid1 = T.nn.Linear(num_features, 8)  # 4-(8-8)-1\n",
    "\t\tself.hid2 = T.nn.Linear(8, 8)\n",
    "\t\tself.oupt = T.nn.Linear(8, 1)\n",
    "\n",
    "\t\tT.nn.init.xavier_uniform_(self.hid1.weight) \n",
    "\t\tT.nn.init.zeros_(self.hid1.bias)\n",
    "\t\tT.nn.init.xavier_uniform_(self.hid2.weight) \n",
    "\t\tT.nn.init.zeros_(self.hid2.bias)\n",
    "\t\tT.nn.init.xavier_uniform_(self.oupt.weight) \n",
    "\t\tT.nn.init.zeros_(self.oupt.bias)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tz = T.tanh(self.hid1(x)) \n",
    "\t\tz = T.tanh(self.hid2(z))\n",
    "\t\tz = T.sigmoid(self.oupt(z)) \n",
    "\t\treturn z\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "def main():\n",
    "\t# 0. get started\n",
    "\tprint(\"\\nBanknote authentication using PyTorch \\n\")\n",
    "\tT.manual_seed(1)\n",
    "\tnp.random.seed(1)\n",
    "\n",
    "\t# 1. create Dataset and DataLoader objects\n",
    "\tprint(\"Creating Banknote train and test DataLoader \")\n",
    "\n",
    "# \ttrain_file = \"./banknote_k20_train.txt\"\n",
    "# \ttest_file = \"./banknote_k20_test.txt\"\n",
    "\n",
    "# \ttrain_ds = BanknoteDataset(train_file)  # all rows\n",
    "# \ttest_ds = BanknoteDataset(test_file)\n",
    "\n",
    "\ttrain_ds = BestDataset(\"train.csv\")\n",
    "\ttest_ds = BestDataset(\"train.csv\")# all rows\n",
    "\n",
    "\tbat_size = 10\n",
    "\ttrain_ldr = T.utils.data.DataLoader(train_ds,\n",
    "\t\tbatch_size=bat_size, shuffle=True)\n",
    "\t# test_ldr = T.utils.data.DataLoader(test_ds,\n",
    "\t#   batch_size=1, shuffle=False)  # not needed\n",
    "\n",
    "\t# 2. create neural network\n",
    "\tprint(\"Creating 4-(8-8)-1 binary NN classifier \")\n",
    "\tnet = Net().to(device)\n",
    "\n",
    "\t# 3. train network\n",
    "\tprint(\"\\nPreparing training\")\n",
    "\tnet = net.train()  # set training mode\n",
    "\tlrn_rate = 0.01\n",
    "\tloss_obj = T.nn.BCELoss()  # binary cross entropy\n",
    "\toptimizer = T.optim.SGD(net.parameters(),\n",
    "\t\tlr=lrn_rate)\n",
    "\tmax_epochs = 100\n",
    "\tep_log_interval = 10\n",
    "\tprint(\"Loss function: \" + str(loss_obj))\n",
    "\tprint(\"Optimizer: SGD\")\n",
    "\tprint(\"Learn rate: 0.01\")\n",
    "\tprint(\"Batch size: 10\")\n",
    "\tprint(\"Max epochs: \" + str(max_epochs))\n",
    "\n",
    "\tprint(\"\\nStarting training\")\n",
    "\tfor epoch in range(0, max_epochs):\n",
    "\t\tepoch_loss = 0.0            # for one full epoch\n",
    "\t\tepoch_loss_custom = 0.0\n",
    "\t\tnum_lines_read = 0\n",
    "\n",
    "\t\tfor (batch_idx, batch) in enumerate(train_ldr):\n",
    "\t\t\tX = batch['predictors']  # [10,4]  inputs\n",
    "\t\t\tY = batch['target']      # [10,1]  targets\n",
    "\t\t\toupt = net(X)            # [10,1]  computeds \n",
    "\n",
    "\t\t\tloss_val = loss_obj(oupt, Y)   # a tensor\n",
    "\t\t\tepoch_loss += loss_val.item()  # accumulate\n",
    "\t\t\t# epoch_loss += loss_val  # is OK\n",
    "\t\t\t# epoch_loss_custom += my_bce(net, batch)\n",
    "\n",
    "\t\t\toptimizer.zero_grad() # reset all gradients\n",
    "\t\t\tloss_val.backward()   # compute all gradients\n",
    "\t\t\toptimizer.step()      # update all weights\n",
    "\n",
    "\t\tif epoch % ep_log_interval == 0:\n",
    "\t\t\tprint(\"epoch = %4d   loss = %0.4f\" % \\\n",
    "\t\t\t\t(epoch, epoch_loss))\n",
    "\t\t\t# print(\"custom loss = %0.4f\" % epoch_loss_custom)\n",
    "\t\t\t# print(\"\")\n",
    "\tprint(\"Done \")\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "\t# 4. evaluate model\n",
    "\tnet = net.eval()\n",
    "\tacc_train = accuracy(net, train_ds)\n",
    "\tprint(\"\\nAccuracy on train data = %0.2f%%\" % \\\n",
    "\t\t(acc_train * 100))\n",
    "\tacc_test = accuracy(net, test_ds)\n",
    "\tprint(\"Accuracy on test data = %0.2f%%\" % \\\n",
    "\t\t(acc_test * 100))\n",
    "\n",
    "\t# acc_train_c = acc_coarse(net, train_ds)\n",
    "\t# print(\"Accuracy on train data = %0.2f%%\" % \\\n",
    "\t#  (acc_train_c * 100))\n",
    "\t# acc_test_c = acc_coarse(net, test_ds)\n",
    "\t# print(\"Accuracy on test data = %0.2f%%\" % \\\n",
    "\t#  (acc_test_c * 100))\n",
    "\n",
    "\t# 5. save model\n",
    "\tprint(\"\\nSaving trained model state_dict \\n\")\n",
    "\tpath = \"./Models/banknote_sd_model.pth\"\n",
    "\tT.save(net.state_dict(), path)\n",
    "\n",
    "\t# print(\"\\nSaving entire model \\n\")\n",
    "\t# path = \".\\\\Models\\\\banknote_full_model.pth\"\n",
    "\t# T.save(net, path\n",
    "\n",
    "\t# print(\"\\nSaving trained model as ONNX \\n\")\n",
    "\t# path = \".\\\\Models\\\\banknote_onnx_model.onnx\"\n",
    "\t# dummy = T.tensor([[0.5, 0.5, 0.5, 0.5]],\n",
    "\t#   dtype=T.float32).to(device)\n",
    "\t# T.onnx.export(net, dummy, path,\n",
    "\t#   input_names=[\"input1\"],\n",
    "\t#  output_names=[\"output1\"])\n",
    "\n",
    "\t# model = Net()  # later . . \n",
    "\t# model.load_state_dict(T.load(path))\n",
    "\n",
    "\t# 6. make a prediction \n",
    "\traw_inpt = np.array([[3.616920113563537598e-01,1.101886108517646790e-01,3.644079566001892090e-01,1.031319573521613936e-01,3.566441237926483154e-01,1.021841317415237427e-01,3.653726577758789062e-01,1.102560833096504211e-01,3.445286750793457031e-01,1.060461923480034013e-01,3.619174063205718994e-01,1.528370529413223267e-01,3.323471248149871826e-01,1.449386030435562134e-01,3.669901788234711248e-01,2.062972038984298706e-01,3.310599625110626221e-01,1.974428743124007901e-01,3.743345141410828192e-01,2.112662494182586670e-01,3.636945784091949463e-01,2.091583311557769498e-01,3.517091572284698486e-01,2.645420432090759277e-01,3.245990872383117676e-01,2.612654566764831543e-01,3.574394583702087402e-01,3.504534065723419189e-01,3.154499828815460205e-01,3.497340679168701172e-01,3.604080677032471258e-01,4.269456267356873114e-01,3.118868470191956122e-01,4.375603199005126953e-01,3.467712104320526123e-01,1.484414786100387573e-01]],\n",
    "\t\tdtype=np.float32) # should be crossing\n",
    "\tnorm_inpt = raw_inpt / 20\n",
    "\tunknown = T.tensor(norm_inpt,\n",
    "\t\tdtype=T.float32).to(device) \n",
    "\n",
    "\tprint(\"Setting normalized inputs to:\")\n",
    "\tfor x in unknown[0]:\n",
    "\t\tprint(\"%0.3f \" % x, end=\"\")\n",
    "\n",
    "\tnet = net.eval()\n",
    "\twith T.no_grad():\n",
    "\t\traw_out = net(unknown)    # a Tensor\n",
    "\tpred_prob = raw_out.item()  # scalar, [0.0, 1.0]\n",
    "\n",
    "\tprint(\"\\nPrediction prob = %0.6f \" % pred_prob)\n",
    "\tif pred_prob < 0.5:\n",
    "\t\tprint(\"Prediction = crossing\")\n",
    "\telse:\n",
    "\t\tprint(\"Prediction = not_crossing\")\n",
    "\n",
    "\tprint(\"\\nEnd Banknote demo\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Banknote authentication using PyTorch \n",
      "\n",
      "Creating Banknote train and test DataLoader \n",
      "37\n",
      "37\n",
      "Creating 4-(8-8)-1 binary NN classifier \n",
      "\n",
      "Preparing training\n",
      "Loss function: BCELoss()\n",
      "Optimizer: SGD\n",
      "Learn rate: 0.01\n",
      "Batch size: 10\n",
      "Max epochs: 100\n",
      "\n",
      "Starting training\n",
      "epoch =    0   loss = 24.9639\n",
      "epoch =   10   loss = 23.1978\n",
      "epoch =   20   loss = 23.0646\n",
      "epoch =   30   loss = 22.8904\n",
      "epoch =   40   loss = 22.7126\n",
      "epoch =   50   loss = 22.4488\n",
      "epoch =   60   loss = 22.1473\n",
      "epoch =   70   loss = 21.8326\n",
      "epoch =   80   loss = 21.3243\n",
      "epoch =   90   loss = 21.0443\n",
      "Done \n",
      "\n",
      "Accuracy on train data = 73.24%\n",
      "Accuracy on test data = 73.24%\n",
      "\n",
      "Saving trained model state_dict \n",
      "\n",
      "Setting normalized inputs to:\n",
      "0.018 0.006 0.018 0.005 0.018 0.005 0.018 0.006 0.017 0.005 0.018 0.008 0.017 0.007 0.018 0.010 0.017 0.010 0.019 0.011 0.018 0.010 0.018 0.013 0.016 0.013 0.018 0.018 0.016 0.017 0.018 0.021 0.016 0.022 0.017 0.007 \n",
      "Prediction prob = 0.859546 \n",
      "Prediction = not_crossing\n",
      "\n",
      "End Banknote demo\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__== \"__main__\":\n",
    "\tmain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
